{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGGZ_TqveO7A",
        "outputId": "485ff529-2919-4585-a7b5-5ff029915da6"
      },
      "outputs": [],
      "source": [
        "##MAKE SURE ALL EXTERNAL LIBRARIES ARE INSTALLED\n",
        "\n",
        "# !pip install pandas\n",
        "# !pip install matplotlib\n",
        "# !pip install seaborn\n",
        "# !pip install wordcloud\n",
        "# !pip install torch\n",
        "# !pip install transformers\n",
        "# !pip install regex\n",
        "# !pip install datasets\n",
        "# !pip install pyLDAvis\n",
        "# !pip install scikit-learn\n",
        "# !pip install plotly\n",
        "# !pip install tqdm\n",
        "# !pip install yfinance\n",
        "# !pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import plotly.graph_objects as go\n",
        "import yfinance as yf\n",
        "import plotly.io as pio\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "from operator import itemgetter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "13d309be672248c9958dfd428d292b9b",
            "1e40baa3bcbb4472a3eab222f2e0cfde",
            "0f3e62d774e44cbca9f7eee1d877847c",
            "12224749ef8b4d2fbc4d8d244e4e1ab3",
            "1b4012066d7b4d7580a5c5e1998ced34",
            "ae60a85919484dc3b8760674da98ebb8",
            "da82c2bc09314bdb827dcb2d1938e939",
            "63af6e7c734c451db13fce7bd962a6b5",
            "65b07e055f424b38b1eca25fefbb0df7",
            "101e73ea71e848b2a21b6cd02e26e2da",
            "69bd000913fe43b0a82096f30254e121",
            "48b1ca387c0a478f92f40500017a3618",
            "64864dafec914310b816ff33bf0c51d1",
            "f392e7b176a24906a5589540ba672123",
            "d6fd943ff4a9440fb626a22003a8ac12",
            "6b244f4db4494f049373f66d14d517f8"
          ]
        },
        "id": "t7MIMxllebpL",
        "outputId": "a131b327-a968-4c65-e789-e6bed3da65be"
      },
      "outputs": [],
      "source": [
        "#LOAD DATASET FROM HUGGINFFACE\n",
        "\n",
        "dataset = load_dataset('mjw/stock_market_tweets')\n",
        "df = dataset['train'].to_pandas()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0GvlCDAfYjf"
      },
      "outputs": [],
      "source": [
        "#BASIC INFORMATION ABOUT THE DATASET\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Column names: {list(df.columns)}\")\n",
        "print(f\"The starting date of the dataset: {df['post_date'].min()} and the ending date of the dataset: {df['post_date'].max()}\")\n",
        "\n",
        "#NA CHECK\n",
        "\n",
        "print(\"The numbers of missing value for each columns:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB4Mr9z_fbZS",
        "outputId": "d215250b-2f79-484f-b000-ac0462f83883"
      },
      "outputs": [],
      "source": [
        "#INITIAL CLEANUP\n",
        "\n",
        "df = df.drop(['Unnamed: 0'], axis=1)\n",
        "df = df.dropna()\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TYPECASTING\n",
        "\n",
        "df['comment_num'] = df['comment_num'].astype(int)\n",
        "df['retweet_num'] = df['retweet_num'].astype(int)\n",
        "df['like_num'] = df['like_num'].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TWEET PREPROCESSING\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#RANGE OF EACH COLUMN\n",
        "\n",
        "print(f\"Range of comment_num: {df['comment_num'].min()} - {df['comment_num'].max()}\")\n",
        "print(f\"Range of retweet_num: {df['retweet_num'].min()} - {df['retweet_num'].max()}\")\n",
        "print(f\"Range of like_num: {df['like_num'].min()} - {df['like_num'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3F-K-1I8fd2J",
        "outputId": "4d84932f-8e5d-4680-ea04-561b21eae9c2"
      },
      "outputs": [],
      "source": [
        "#EXPLORE THE DISTRIBUTION OF NUMERIC COLUMNS \n",
        "\n",
        "import plotly.express as px\n",
        "for col in ['comment_num', 'retweet_num', 'like_num']:\n",
        "    fig = px.histogram(df, x=col, nbins=30)\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "8pHxTPZmfkBb",
        "outputId": "86c32071-5326-49a1-89b1-e95944f9f45f"
      },
      "outputs": [],
      "source": [
        "#EXPLORE THE DISTRIBUTION OF THE READILY AVAILABLE TICKER SYMBOLS\n",
        "fig = px.histogram(df, x='ticker_symbol')\n",
        "fig.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "APPLY PRETRAINED MODEL TO OBTAIN SENTIMENT LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAPrX5s1gBpY"
      },
      "outputs": [],
      "source": [
        "## LEVERAGE UPON THE GPU FACILITY OF GOOGLE COLAB TO APPLY THE PRETRAINED MODEL\n",
        "\n",
        "# import torch\n",
        "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "# from tqdm import tqdm\n",
        "\n",
        "##CHECK IF GPU IS AVAILABLE\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## MODEL AND TOKENIZER LOADING\n",
        "# model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "##LABELS MAPPING\n",
        "# labels = {'LABEL_2': 1, 'LABEL_1': 0, 'LABEL_0': -1}\n",
        "\n",
        "# def sentiment_analysis(texts):\n",
        "#     #INPUT PROCESSING\n",
        "#     inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "#     inputs = inputs.to(device) \n",
        "#     #EXTRACT PREDICTIONS\n",
        "#     with torch.no_grad():  \n",
        "#         predictions = model(**inputs)\n",
        "#     _, predicted = torch.max(predictions.logits, 1)\n",
        "#     return [labels[f'LABEL_{p.item()}'] for p in predicted]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #APPLY THE MODEL TO THE DATASET\n",
        "# batch_size = 16 \n",
        "# sentiments = []\n",
        "# for i in tqdm(range(0, len(df), batch_size)):\n",
        "#     batch = df['body'].iloc[i:i+batch_size].tolist()\n",
        "#     batch_sentiments = sentiment_analysis(batch)\n",
        "#     sentiments.extend(batch_sentiments)\n",
        "# #BUILD DF\n",
        "# df['sentiment'] = sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm8P9C2rr0vo"
      },
      "outputs": [],
      "source": [
        "##DUMPT THE NEW DATASET WITH CLASSIFIFIED LABELS INTO A CSV\n",
        "#df.to_csv('cached_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "M6EivzXuhJSR",
        "outputId": "d1b560e9-4271-4685-9c01-715a03313a90"
      },
      "outputs": [],
      "source": [
        "## EARLIER THIS CODE SNIPPET IS NECESSARY WHEN USING GOOGLE COLAB\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv('/content/drive/My Drive/Quantallia Analyst Notes/cached_labels.csv')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8OUAvoBtsftH",
        "outputId": "7feb9f4a-f3c5-4d08-8806-98633a54b724"
      },
      "outputs": [],
      "source": [
        "##FOR REPRODUCIBILITY, ENSURE YOU EDIT THIS PATH\n",
        "path = \"C:\\\\Users\\\\hajiw\\\\Downloads\\\\cached_labels.csv\"\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DW8LvxsEFS",
        "outputId": "161ce085-77e1-423e-8347-fd14a203847c"
      },
      "outputs": [],
      "source": [
        "#SENTIMENT DISTRIBUTION ACROSS TICKER\n",
        "df_sentiment = df.groupby(['ticker_symbol', 'sentiment']).size().reset_index(name='counts')\n",
        "color_sequence = ['yellow', 'green', 'red']\n",
        "\n",
        "fig = px.pie(df_sentiment, names='sentiment', values='counts', facet_col='ticker_symbol', \n",
        "             title='Sentiment Distribution per Ticker Symbol', \n",
        "             labels={'counts':'Count', 'sentiment':'Sentiment', 'ticker_symbol':'Ticker Symbol'},\n",
        "             color_discrete_sequence=color_sequence)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eI2VCskvr-Gm",
        "outputId": "ed7b95e8-7a52-4d9d-b826-27f770e28727"
      },
      "outputs": [],
      "source": [
        "##PLOTS OF DAILY, AVERAGE SENTIMENT SCORES FOR EACH TICKER SYMBOL OVER SAMPLED PERIOD\n",
        "\n",
        "df['post_date'] = pd.to_datetime(df['post_date'])\n",
        "grouped_df = df.groupby('ticker_symbol')\n",
        "\n",
        "# Construct time series of average daily sentiment scores for each ticker\n",
        "for ticker, group in grouped_df:\n",
        "    # Sort the tweets by 'post_date'\n",
        "    group = group.sort_values('post_date')\n",
        "\n",
        "    # Resample the sorted group to daily level and take the mean of the sentiment scores\n",
        "    daily_sentiment = group.resample('D', on='post_date')['sentiment'].mean()\n",
        "    \n",
        "    # Create a figure for the current ticker\n",
        "    fig = go.Figure(data=go.Scatter(x=daily_sentiment.index, y=daily_sentiment.values))\n",
        "    \n",
        "    # Set layout options for the current plot\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(title='Date'),\n",
        "        yaxis=dict(title='Average Daily Sentiment Score'),\n",
        "        title=f'Average Daily Sentiment Score over Time - {ticker}'\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRADING STRATEGY BACKTESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##PREPARE THE CLOSE PRICE FOR BACKTETING\n",
        "\n",
        "#INPUT TICKERS TO MAKE REQUESTS TO YFINANCE \n",
        "tickers = df['ticker_symbol'].dropna().unique().tolist()\n",
        "#EXTRACT CLOSE PRICE USING .DOWNLOAD()\n",
        "close_prices = yf.download(tickers, start=df['post_date'].min(), end=df['post_date'].max())['Close']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CREATING A NEW DATAFRAME FOR AVERAGE DAILY SENTIMENT SCORES\n",
        "\n",
        "sentiment_df = df.groupby(['ticker_symbol', pd.Grouper(key='post_date', freq='D')])['sentiment'].mean().reset_index()\n",
        "\n",
        "#UNSTACKING THE MULTI-INDEX DATAFRAME TO MAKE IT READY TO MERGE WITH CLOSE_PRICES\n",
        "\n",
        "sentiment_df = sentiment_df.pivot(index='post_date', columns='ticker_symbol', values='sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKChiWJBx53K"
      },
      "outputs": [],
      "source": [
        "### RERUNNING THIS SCRIPT CAN BE EXTREMELY TIME-CONSUMING.\n",
        "\n",
        "\n",
        "\n",
        "### PREPARE AUGMENTED DF FOR BACKTESTING\n",
        "\n",
        "# augmented_df = pd.concat([close_prices, sentiment_df], keys=['Close', 'Sentiment'], axis=1).sort_index(axis=1)\n",
        "# augmented_df = augmented_df.sort_index()  \n",
        "# #Calculate sentiment score difference over 3-day period\n",
        "# for ticker in tickers:\n",
        "#     if ticker in augmented_df.columns.get_level_values(1):\n",
        "#         augmented_df[('Signal', ticker)] = augmented_df[('Sentiment', ticker)].diff(3)\n",
        "# # Create a DataFrame to store positions\n",
        "# positions_df = pd.DataFrame(index=augmented_df.index)\n",
        "\n",
        "# # If the sentiment scores increase for three days in a row, we buy (long position = 1)\n",
        "# # If the sentiment scores decrease for three days in a row, we sell (short position = -1)\n",
        "# # If the sentiment scores do not consistently increase or decrease, we do nothing (position = 0)\n",
        "# for ticker in tickers:\n",
        "#     if ticker in augmented_df.columns.get_level_values(1):\n",
        "#         positions_df[ticker] = np.where(augmented_df[('Signal', ticker)] > 0, 1, 0)\n",
        "#         positions_df[ticker] = np.where(augmented_df[('Signal', ticker)] < 0, -1, positions_df[ticker])\n",
        "\n",
        "# #Forward fill positions to simulate holding positions until next trade\n",
        "# positions_df.ffill(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVRRFRWlyAKz"
      },
      "outputs": [],
      "source": [
        "##1/ CALCULATE DAILY LOG RETURNS OF THE CLOSE PRICES\n",
        "\n",
        "# for ticker in tickers:\n",
        "#     if ticker in augmented_df.columns.get_level_values(1):\n",
        "#         augmented_df[('Log Returns', ticker)] = np.log(augmented_df[('Close', ticker)] / augmented_df[('Close', ticker)].shift(1))\n",
        "\n",
        "##2/ PREPARE LOG RETURNS DATAFRAME\n",
        "\n",
        "# log_returns_df = augmented_df.xs('Log Returns', axis=1, level=0, drop_level=False)\n",
        "# log_returns_df.columns = log_returns_df.columns.droplevel(0)\n",
        "\n",
        "##3/ CALCULATE DAILY STRATEGY RETURNS\n",
        "\n",
        "# strategy_returns_df = positions_df.shift().reindex(log_returns_df.index) * log_returns_df\n",
        "\n",
        "##4/ CALCULATE CUMULATIVE STRATEGY RETURNS\n",
        "\n",
        "# cumulative_strategy_returns_df = strategy_returns_df.cumsum()\n",
        "\n",
        "##5/ RETURN THE CUMULATIVE RETURNS OF THE STRATEGY\n",
        "\n",
        "# print(cumulative_strategy_returns_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CONVERTING LOG RETURNS TO SIMPLE RETURNS MANUALLY SINCE I FORGOT TO SAVE THE AUGMENTED_DF (RESULTS ARE SCREENSHOTTED)\n",
        "\n",
        "# ticker_cumreturns = {'AAPL': 0.75183, 'AMZN': 2.107555, 'GOOG': -0.263453, 'MSFT': -1.082538, 'TSLA': 20.471324, 'SP500':0.450892}\n",
        "# initial_investment = 1000\n",
        "\n",
        "# for ticker, log_return in ticker_cumreturns.items():\n",
        "#     simple_return = np.exp(log_return) - 1\n",
        "#     final_value = initial_investment * (1 + simple_return)\n",
        "#     print(f\"{ticker}: {final_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CALCULATE THE EXPECTED PERFORMANCE OF AN EQUALLY-WEIGHTED PORTFOLIO\n",
        "\n",
        "# initial_investment_per_asset = 1000 / len(ticker_cumreturns)\n",
        "# total_value = 0\n",
        "# for ticker, log_return in ticker_cumreturns.items():\n",
        "#     simple_return = np.exp(log_return) - 1\n",
        "#     final_value = initial_investment_per_asset * (1 + simple_return)\n",
        "#     total_value += final_value\n",
        "#     print(f\"Final value of {ticker}: ${final_value:.2f}\")\n",
        "\n",
        "# total_value = int(total_value)\n",
        "# print(f\"Total value of the portfolio: ${total_value:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBWhReMyy63K",
        "outputId": "2418f51f-f7d2-45fa-feb0-060c7c181700"
      },
      "outputs": [],
      "source": [
        "## RETURN THE BACKTEST STATISTICS\n",
        "\n",
        "# print(f'Backtest Timeframe: {augmented_df.index[0].date()} to {augmented_df.index[-1].date()}')\n",
        "\n",
        "##  Calculate number of trades entered\n",
        "# num_trades = np.count_nonzero(positions_df.diff().abs())\n",
        "\n",
        "## Calculate number of winning trades\n",
        "# num_winning_trades = np.count_nonzero((strategy_returns_df > 0).sum(axis=1))\n",
        "\n",
        "## Calculate win rate\n",
        "# win_rate = num_winning_trades / num_trades\n",
        "\n",
        "# print(f'Number of trades entered: {num_trades}')\n",
        "# print(f'Number of winning trades: {num_winning_trades}')\n",
        "# print(f'Win rate: {win_rate:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##CALCULATION SP500 LOG RETURNS OVER THE SAMPLED PERIOD\n",
        "# data = yf.download('^GSPC', start='2015-01-01', end='2020-01-01')\n",
        "# data['Log Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "# data['Cumulative Log Returns of SP500'] = data['Log Returns'].cumsum()\n",
        "# print(data['Cumulative Log Returns of SP500'].tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##CALCULATE THE CASH TERMS FOR THE STRATEGY'S RETURNS.\n",
        "\n",
        "# initial_investment = 1000\n",
        "# ticker_cumreturns = {'AAPL': 0.75183, 'AMZN': 2.107555, 'GOOG': -0.263453, 'MSFT': -1.082538, 'TSLA': 20.471324}\n",
        "# final_value = {}\n",
        "# for ticker, cum_return in ticker_cumreturns.items():\n",
        "#     final_value[ticker] = initial_investment * (1 + cum_return)\n",
        "# print(\"Final value of the investment for each stock, given an initial investment of $1000:\")\n",
        "# for ticker, value in final_value.items():\n",
        "#     print(f\"{ticker}: ${value:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## IDENTIFICATION OF LONGEST PERIOD OF POSITIVE/NEGATIVE SENTIMENT SCORES FOR EACH TICKER SYMBOL\n",
        "\n",
        "    \n",
        "# Make sure 'post_date' is a datetime object\n",
        "df['post_date'] = pd.to_datetime(df['post_date'])\n",
        "# Group the DataFrame by 'ticker_symbol'\n",
        "grouped_df = df.groupby('ticker_symbol')\n",
        "# Initialize variables to store the longest positive and negative periods for each ticker\n",
        "positive_periods = {}\n",
        "negative_periods = {}\n",
        "\n",
        "# Construct time series of average daily sentiment scores for each ticker\n",
        "for ticker, group in grouped_df:\n",
        "    group = group.sort_values('post_date')\n",
        "    daily_sentiment = group.resample('D', on='post_date')['sentiment'].mean()\n",
        "\n",
        "    # Find the periods when sentiment scores remain above 0\n",
        "    positive_mask = daily_sentiment > 0\n",
        "    positive_runs = (positive_mask != positive_mask.shift()).cumsum()\n",
        "    positive_periods[ticker] = daily_sentiment[positive_mask].groupby(positive_runs).apply(lambda x: (x.index[0], x.index[-1]))\n",
        "\n",
        "    # Find the periods when sentiment scores remain below 0\n",
        "    negative_mask = daily_sentiment < 0\n",
        "    negative_runs = (negative_mask != negative_mask.shift()).cumsum()\n",
        "    negative_periods[ticker] = daily_sentiment[negative_mask].groupby(negative_runs).apply(lambda x: (x.index[0], x.index[-1]))\n",
        "\n",
        "for ticker in positive_periods:\n",
        "    longest_positive_period = max(positive_periods[ticker], key=lambda x: x[1] - x[0])\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    print(f\"Longest positive period: {longest_positive_period[0].date()} to {longest_positive_period[1].date()}\")\n",
        "\n",
        "for ticker in negative_periods:\n",
        "    longest_negative_period = max(negative_periods[ticker], key=lambda x: x[1] - x[0])\n",
        "    print(f\"Ticker: {ticker}\")\n",
        "    print(f\"Longest negative period: {longest_negative_period[0].date()} to {longest_negative_period[1].date()}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
